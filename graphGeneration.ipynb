{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int = {'Benign': 0, 'InSitu': 1, 'Invasive': 2, 'Normal': 3}\n",
    "# ['Benign', 'InSitu', 'Invasive', 'Normal']\n",
    "root_dir = '../data/Photos/'\n",
    "patches = 'Patches'\n",
    "graph_dir_delaunay = './graphs_delaunay/'\n",
    "graph_dir_knn = './graphs_knn/'\n",
    "\n",
    "os.makedirs(graph_dir_delaunay, exist_ok=True)\n",
    "os.makedirs(graph_dir_knn, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "# Load ViT-B/16 with ImageNet pretrained weights\n",
    "model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace the classification head to output 4 classes\n",
    "model.heads = torch.nn.Linear(model.heads.head.in_features, 4)\n",
    "\n",
    "model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "def blur_except_subpatch(img_tensor, subpatch_idx, patch_size=56):\n",
    "    img_pil = TF.to_pil_image(img_tensor.squeeze().cpu())\n",
    "    img_cv2 = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "    blurred = cv2.GaussianBlur(img_cv2, (25, 25), 0)\n",
    "\n",
    "    row, col = divmod(subpatch_idx, 4)\n",
    "    r_start, c_start = row * patch_size, col * patch_size\n",
    "    blurred[r_start:r_start+patch_size, c_start:c_start+patch_size, :] = \\\n",
    "        img_cv2[r_start:r_start+patch_size, c_start:c_start+patch_size, :]\n",
    "\n",
    "    img_blurred = Image.fromarray(cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB))\n",
    "    return transform(img_blurred).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_delaunay(node_feats, centroids):\n",
    "    pos = np.array(centroids)\n",
    "    x = torch.tensor(node_feats, dtype=torch.float)\n",
    "    tri = Delaunay(pos)\n",
    "\n",
    "    edges = set()\n",
    "    for simplex in tri.simplices:\n",
    "        for i in range(3):\n",
    "            for j in range(i+1, 3):\n",
    "                u, v = simplex[i], simplex[j]\n",
    "                edges.add(tuple(sorted((u, v))))\n",
    "\n",
    "    edge_index, edge_attr = [], []\n",
    "    cos_sim = cosine_similarity(x)\n",
    "\n",
    "    for u, v in edges:\n",
    "        dist = np.linalg.norm(pos[u] - pos[v])\n",
    "        sim = (1.0 / (dist + 1e-6) + cos_sim[u][v]) / 2\n",
    "        edge_index += [[u, v], [v, u]]\n",
    "        edge_attr += [sim, sim]\n",
    "\n",
    "    return Data(\n",
    "        x=x,\n",
    "        edge_index=torch.tensor(edge_index).T,\n",
    "        edge_attr=torch.tensor(edge_attr),\n",
    "        pos=torch.tensor(pos, dtype=torch.float)\n",
    "    )\n",
    "\n",
    "def build_graph_knn(node_feats, centroids, k=4):\n",
    "    pos = np.array(centroids)\n",
    "    x = torch.tensor(node_feats, dtype=torch.float)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(pos)\n",
    "    _, indices = nbrs.kneighbors(pos)\n",
    "\n",
    "    edge_index, edge_attr = [], []\n",
    "    cos_sim = cosine_similarity(x)\n",
    "\n",
    "    for i in range(len(pos)):\n",
    "        for j in indices[i][1:]:\n",
    "            dist = np.linalg.norm(pos[i] - pos[j])\n",
    "            sim = (1.0 / (dist + 1e-6) + cos_sim[i][j]) / 2\n",
    "            edge_index += [[i, j], [j, i]]\n",
    "            edge_attr += [sim, sim]\n",
    "\n",
    "    return Data(\n",
    "        x=x,\n",
    "        edge_index=torch.tensor(edge_index).T,\n",
    "        edge_attr=torch.tensor(edge_attr),\n",
    "        pos=torch.tensor(pos, dtype=torch.float)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistopathologyPatchDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform, label_map):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "        for class_name in os.listdir(root_dir):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            print(class_name, class_path)\n",
    "            if not os.path.isdir(class_path): continue\n",
    "            for path in glob(os.path.join(class_path, \"*.tif\")):\n",
    "                # print(path)\n",
    "                self.samples.append((path, class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_name = self.samples[idx]\n",
    "        label = self.label_map[label_name]\n",
    "        return img_path, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Your dataset, model, transform, device, etc. must already be defined\n",
    "\n",
    "print(root_dir)\n",
    "dataset = HistopathologyPatchDataset(root_dir, transform, label_to_int)\n",
    "print(dataset.samples)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "csv_delaunay = []\n",
    "csv_knn = []\n",
    "\n",
    "for img_path, label in tqdm(loader):\n",
    "    img_path = img_path[0]\n",
    "    label = label.item()\n",
    "\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "        feats, coords = []\n",
    "\n",
    "        correct_preds = 0\n",
    "        total_patches = 0\n",
    "\n",
    "        for y in range(0, H - 224 + 1, 224):\n",
    "            for x in range(0, W - 224 + 1, 224):\n",
    "                patch = image.crop((x, y, x+224, y+224))\n",
    "                patch_tensor = transform(patch).unsqueeze(0).to(device)\n",
    "\n",
    "                for i in range(16):\n",
    "                    sub_tensor = blur_except_subpatch(patch_tensor, i)\n",
    "                    with torch.no_grad():\n",
    "                        _ = model(sub_tensor)\n",
    "                        pred = torch.argmax(torch.softmax(_, dim=1)).item()\n",
    "                        print(f\"Patch {i} prediction: {pred}, label: {label}\")\n",
    "                    total_patches += 1\n",
    "                    if pred == label:\n",
    "                        correct_preds += 1\n",
    "                        feats.append(features_dict['feat'].squeeze().cpu().numpy())\n",
    "                        row, col = divmod(i, 4)\n",
    "                        cx, cy = x + col*56 + 28, y + row*56 + 28\n",
    "                        coords.append((cx, cy))\n",
    "\n",
    "#         if len(feats) < 3:\n",
    "#             continue\n",
    "\n",
    "#         graph_d = build_graph_delaunay(feats, coords)\n",
    "#         graph_k = build_graph_knn(feats, coords, k=4)\n",
    "\n",
    "#         fname = os.path.splitext(os.path.basename(img_path))[0]\n",
    "#         path_d = os.path.join(graph_dir_delaunay, f\"{fname}.pt\")\n",
    "#         path_k = os.path.join(graph_dir_knn, f\"{fname}.pt\")\n",
    "\n",
    "#         torch.save(graph_d, path_d)\n",
    "#         torch.save(graph_k, path_k)\n",
    "\n",
    "#         accuracy = correct_preds / total_patches if total_patches > 0 else 0.0\n",
    "\n",
    "#         csv_delaunay.append({\n",
    "#             'graph_path': path_d,\n",
    "#             'label': label,\n",
    "#             'accuracy': round(accuracy, 4)\n",
    "#         })\n",
    "#         csv_knn.append({\n",
    "#             'graph_path': path_k,\n",
    "#             'label': label,\n",
    "#             'accuracy': round(accuracy, 4)\n",
    "#         })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {img_path} — {e}\")\n",
    "\n",
    "# # Save CSV metadata\n",
    "# pd.DataFrame(csv_delaunay).to_csv(\"d_meta.csv\", index=False)\n",
    "# pd.DataFrame(csv_knn).to_csv(\"k_meta.csv\", index=False)\n",
    "\n",
    "# print(\"✅ Saved metadata to d_meta.csv and k_meta.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
