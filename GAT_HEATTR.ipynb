{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "54a556da",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 1 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5d221359",
            "metadata": {},
            "source": [
                "# Graph Attention Network for Histopathology Attribution\n",
                "## Complete Graph with Attention-based Edge Pruning\n",
                "\n",
                "**Architecture:**\n",
                "- Patches as nodes with ViT features (768-D)\n",
                "- Complete graph initialization (all nodes connected)\n",
                "- Edge weights = 1 / Euclidean distance (spatial proximity)\n",
                "- Node attention score = average of edge attention scores\n",
                "- Dynamic edge pruning based on attention threshold\n",
                "- Classification from graph-level features"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aabab8f2",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 2 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "71faedd2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
                "from transformers import ViTModel, ViTImageProcessor\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3898146b",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 3 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef826653",
            "metadata": {},
            "source": [
                "## 1. Patch Extraction with Spatial Coordinates"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c179dcfd",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 4 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "084a2d16",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Patch extraction with spatial coordinates defined.\n"
                    ]
                }
            ],
            "source": [
                "def extract_patches_with_coords(image, patch_size=224, stride=112, target_size=224):\n",
                "    \"\"\"\n",
                "    Extract patches with their spatial coordinates for graph construction.\n",
                "    \n",
                "    Returns:\n",
                "        patches: List of PIL images\n",
                "        coordinates: List of (x, y) center coordinates for each patch\n",
                "    \"\"\"\n",
                "    if isinstance(image, str):\n",
                "        image = Image.open(image).convert('RGB')\n",
                "    \n",
                "    width, height = image.size\n",
                "    patches = []\n",
                "    coordinates = []\n",
                "    \n",
                "    for y in range(0, height - patch_size + 1, stride):\n",
                "        for x in range(0, width - patch_size + 1, stride):\n",
                "            patch = image.crop((x, y, x + patch_size, y + patch_size))\n",
                "            patch = patch.resize((target_size, target_size), Image.LANCZOS)\n",
                "            patches.append(patch)\n",
                "            \n",
                "            # Store center coordinates\n",
                "            center_x = x + patch_size // 2\n",
                "            center_y = y + patch_size // 2\n",
                "            coordinates.append((center_x, center_y))\n",
                "    \n",
                "    return patches, np.array(coordinates)\n",
                "\n",
                "print(\"Patch extraction with spatial coordinates defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "88fe5cc5",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 5 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c38d24aa",
            "metadata": {},
            "source": [
                "## 2. ViT Feature Extractor (Frozen Backbone)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a2dbc5e0",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 6 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "4fa17575",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: scikit-image in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (0.25.2)\n",
                        "Requirement already satisfied: numpy>=1.24 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (1.26.4)\n",
                        "Requirement already satisfied: scipy>=1.11.4 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (1.15.3)\n",
                        "Requirement already satisfied: networkx>=3.0 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (3.4.2)\n",
                        "Requirement already satisfied: pillow>=10.1 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (12.1.0)\n",
                        "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (2.37.2)\n",
                        "Requirement already satisfied: tifffile>=2022.8.12 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (2025.5.10)\n",
                        "Requirement already satisfied: packaging>=21 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
                        "Requirement already satisfied: lazy-loader>=0.4 in /home/pclab/miniconda3/envs/clam_latest/lib/python3.10/site-packages (from scikit-image) (0.4)\n"
                    ]
                }
            ],
            "source": [
                "! pip install scikit-image"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed3963dd",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 7 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c4049ec9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ViT Feature Extractor defined.\n"
                    ]
                }
            ],
            "source": [
                "class ViTFeatureExtractor(nn.Module):\n",
                "    \"\"\"Frozen ViT backbone for patch feature extraction.\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
                "        super().__init__()\n",
                "        self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
                "        self.vit = ViTModel.from_pretrained(model_name)\n",
                "        \n",
                "        # Freeze all parameters\n",
                "        for param in self.vit.parameters():\n",
                "            param.requires_grad = False\n",
                "        \n",
                "        self.vit.eval()\n",
                "    \n",
                "    def forward(self, images):\n",
                "        \"\"\"Extract CLS token features from patches.\"\"\"\n",
                "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
                "        inputs = {k: v.to(next(self.vit.parameters()).device) for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.vit(**inputs)\n",
                "            features = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
                "        \n",
                "        return features\n",
                "\n",
                "print(\"ViT Feature Extractor defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "682fc021",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 8 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "dbc4d293",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1 — imports\n",
                "import torch, torch.nn as nn, torch.nn.functional as F\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from torchvision import transforms\n",
                "from skimage.measure import label, regionprops, find_contours\n",
                "from skimage.morphology import remove_small_objects\n",
                "import matplotlib.pyplot as plt\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "640bf864",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 9 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "878b8814",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2 — minimal utilities\n",
                "\n",
                "def attention_rollout(attn_stack, add_residual=True, eps=1e-6):\n",
                "    \"\"\"\n",
                "    attn_stack: list of (H, N, N) attention tensors from L layers after softmax.\n",
                "    Returns (N,) vector of contributions from CLS to all tokens, then reshapes by caller.\n",
                "    \"\"\"\n",
                "    if len(attn_stack) == 0:\n",
                "        return None\n",
                "    A = None\n",
                "    for A_l in attn_stack:\n",
                "        # average heads -> (N, N)\n",
                "        A_l = A_l.mean(0)  # (N, N)\n",
                "        if add_residual:\n",
                "            A_l = A_l + torch.eye(A_l.size(-1), device=A_l.device)\n",
                "            A_l = A_l / A_l.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
                "        A = A_l if A is None else A_l @ A\n",
                "    # CLS is token 0. Return its influence on all tokens.\n",
                "    return A[0]  # (N,)\n",
                "\n",
                "def upsample_to(img_like_hw, fmap_hw, arr_chw):\n",
                "    \"\"\"Bilinear upsample array shaped (C, h, w) to target (H, W).\"\"\"\n",
                "    t = torch.from_numpy(arr_chw).unsqueeze(0)  # (1,C,h,w)\n",
                "    t = F.interpolate(t, size=img_like_hw, mode='bilinear', align_corners=False)\n",
                "    return t.squeeze(0).cpu().numpy()\n",
                "\n",
                "def binarize_instances(seg_logits, thresh=0.5, min_area=20):\n",
                "    \"\"\"seg_logits: (C,H,W) with either 1 channel (foreground) or 2+ for softmax.\"\"\"\n",
                "    if seg_logits.shape[0] == 1:\n",
                "        prob = torch.sigmoid(seg_logits[0]).cpu().numpy()\n",
                "    else:\n",
                "        prob = F.softmax(seg_logits, dim=0)[1].cpu().numpy()  # assume channel 1 = nuclei\n",
                "    mask = prob >= thresh\n",
                "    mask = remove_small_objects(mask, min_size=min_area)\n",
                "    lab = label(mask.astype(np.uint8), connectivity=2)\n",
                "    return lab, prob\n",
                "\n",
                "def assign_types_per_instance(type_logits, inst_map):\n",
                "    \"\"\"type_logits: (K,H,W), inst_map: (H,W) -> dict inst_id -> class_id\"\"\"\n",
                "    if type_logits is None:\n",
                "        return {}\n",
                "    cls = type_logits.softmax(0).argmax(0).cpu().numpy()\n",
                "    out = {}\n",
                "    for k in np.unique(inst_map)[1:]:\n",
                "        m = (inst_map == k)\n",
                "        if m.sum() == 0:\n",
                "            continue\n",
                "        vals = cls[m]\n",
                "        out[int(k)] = int(np.bincount(vals).argmax())\n",
                "    return out\n",
                "\n",
                "def pool_features_over_instances(feat_map_chw, inst_map):\n",
                "    \"\"\"feat_map_chw: (C,H,W) numpy; returns dict inst_id -> (C,) mean pooled feature.\"\"\"\n",
                "    out = {}\n",
                "    C, H, W = feat_map_chw.shape\n",
                "    for k in np.unique(inst_map)[1:]:\n",
                "        m = (inst_map == k)\n",
                "        if m.sum() == 0:\n",
                "            continue\n",
                "        out[int(k)] = feat_map_chw[:, m].mean(axis=1)\n",
                "    return out\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9a47b092",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 10 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8efdd06b",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "5a8fa734",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 11 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "587e7c52",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "d986c47f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Cell 3 — CellViT wrapper\n",
                "# from cellvit.modeling import CellViT256x40\n",
                "# class CellViTSegmentor(nn.Module):\n",
                "#     \"\"\"\n",
                "#     Wraps a CellViT-256x40 instance segmentation model and exposes:\n",
                "#       - nuclei instances with centroids and class ids\n",
                "#       - per-instance pooled embeddings (from last encoder tokens)\n",
                "#       - ViT attention heatmap rolled back to pixels\n",
                "#     You must adapt model construction and internal module paths to your repo.\n",
                "#     \"\"\"\n",
                "#     def __init__(self, weights_path, device='cuda', in_size=256, patch_size=16):\n",
                "#         super().__init__()\n",
                "#         self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
                "#         self.in_size = in_size\n",
                "#         self.patch_size = patch_size\n",
                "\n",
                "#         # --- build model\n",
                "#         # <<< ADAPT: import/construct your exact CellViT-256x40 class\n",
                "#         # from cellvit.modeling import CellViT256x40\n",
                "#         self.model = CellViT256x40(num_types=5)\n",
                "#         # self.model = ...  # <<< ADAPT\n",
                "\n",
                "#         sd = torch.load(weights_path, map_location='cpu')\n",
                "#         self.model.load_state_dict(sd, strict=False)\n",
                "#         self.model.eval().to(self.device)\n",
                "\n",
                "#         # preprocessing; use model's own normalization if provided\n",
                "#         self.tf = transforms.Compose([\n",
                "#             transforms.Resize((in_size, in_size), interpolation=Image.BILINEAR),\n",
                "#             transforms.ToTensor(),\n",
                "#             transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
                "#         ])\n",
                "\n",
                "#         # hooks\n",
                "#         self._last_tokens = None         # (B, N, D) before head/decoder\n",
                "#         self._attn_blocks = []           # list of (H, N, N) tensors across layers\n",
                "\n",
                "#         # <<< ADAPT: attach to your ViT encoder blocks and to the token output you want to pool\n",
                "#         def token_hook(_, __, out):\n",
                "#             # out expected (B, N, D). If it's a tuple, take the token sequence.\n",
                "#             self._last_tokens = out if isinstance(out, torch.Tensor) else out[0]\n",
                "#         def attn_hook(_, __, out):\n",
                "#             # out expected (B, H, N, N) *after* softmax. If you get pre-softmax, apply softmax(dim=-1).\n",
                "#             if isinstance(out, torch.Tensor) and out.dim() == 4:\n",
                "#                 self._attn_blocks.append(out[0])  # store first in batch; adjust if batching\n",
                "#         # Examples of likely paths; change to match your code:\n",
                "#         # self.model.encoder.blocks[-1].register_forward_hook(token_hook)        # <<< ADAPT\n",
                "#         # for blk in self.model.encoder.blocks:                                  # <<< ADAPT\n",
                "#         #     blk.attn.attn_drop.register_forward_hook(attn_hook)                # <<< ADAPT\n",
                "\n",
                "#     @torch.no_grad()\n",
                "#     def _forward_raw(self, pil_images):\n",
                "#         x = torch.stack([self.tf(im) for im in pil_images]).to(self.device)\n",
                "#         self._last_tokens = None\n",
                "#         self._attn_blocks = []\n",
                "#         y = self.model(x)\n",
                "#         # <<< ADAPT: unpack outputs\n",
                "#         # Expecting: seg_logits (B, Cseg, H, W); optional type_logits (B, Ctype, H, W)\n",
                "#         if isinstance(y, dict):\n",
                "#             seg_logits = y['seg_logits']\n",
                "#             type_logits = y.get('type_logits', None)\n",
                "#         elif isinstance(y, (list, tuple)):\n",
                "#             seg_logits = y[0]\n",
                "#             type_logits = y[1] if len(y) > 1 else None\n",
                "#         else:\n",
                "#             seg_logits = y\n",
                "#             type_logits = None\n",
                "#         tokens = self._last_tokens  # (B, N, D) or None\n",
                "#         attn_stack = self._attn_blocks.copy()  # list of (H, N, N)\n",
                "#         return seg_logits, type_logits, tokens, attn_stack\n",
                "\n",
                "#     @torch.no_grad()\n",
                "#     def infer_on_patch(self, pil_patch, return_overlay=True):\n",
                "#         \"\"\"\n",
                "#         pil_patch: PIL.Image (any size). Will be resized to in_size for the model.\n",
                "#         Returns:\n",
                "#           nuclei: list of dicts with centroid_xy (in original patch coords), bbox, area, class_id, feat(np.ndarray)\n",
                "#           attn_map_up: HxW numpy attention heatmap aligned to original patch size\n",
                "#           prob_map_up: nuclei probability map aligned to original patch size\n",
                "#         \"\"\"\n",
                "#         W0, H0 = pil_patch.size\n",
                "#         seg_logits, type_logits, tokens, attn_stack = self._forward_raw([pil_patch])\n",
                "#         seg_logits, type_logits = seg_logits[0], (type_logits[0] if type_logits is not None else None)\n",
                "\n",
                "#         # 1) instance map at model resolution\n",
                "#         inst_map, prob_map = binarize_instances(seg_logits, thresh=0.5, min_area=20)\n",
                "\n",
                "#         # 2) token feature map -> pixel map\n",
                "#         feat_map = None\n",
                "#         if tokens is not None:\n",
                "#             # tokens: (1, N, D) with N = 1 + (in_size/patch)^2. Discard CLS.\n",
                "#             tokens_ = tokens[0]  # (N, D)\n",
                "#             N, D = tokens_.shape\n",
                "#             grid = int((N - 1) ** 0.5)\n",
                "#             patch_tokens = tokens_[1:1 + grid*grid].reshape(grid, grid, D).permute(2,0,1)  # (D, h, w)\n",
                "#             feat_map = patch_tokens.cpu().numpy()  # (D, h, w)\n",
                "#             feat_map = upsample_to((inst_map.shape[0], inst_map.shape[1]), (grid, grid), feat_map)  # (D,H,W)\n",
                "\n",
                "#         # 3) attention rollout -> token grid -> upsample to model H,W -> upsample to original H0,W0\n",
                "#         attn_map_up = None\n",
                "#         if len(attn_stack) > 0:\n",
                "#             a = attention_rollout([t.softmax(-1) if t.max() > 1 else t for t in attn_stack], add_residual=True)\n",
                "#             N = a.shape[0]\n",
                "#             grid = int((N - 1) ** 0.5)\n",
                "#             a_img = a[1:1 + grid*grid].reshape(grid, grid)  # discard CLS\n",
                "#             a_img = a_img / (a_img.max() + 1e-6)\n",
                "#             a_img = a_img.unsqueeze(0).unsqueeze(0)  # (1,1,h,w)\n",
                "#             a_img = F.interpolate(a_img, size=inst_map.shape, mode='bilinear', align_corners=False)[0,0].cpu().numpy()\n",
                "#             a_img = (a_img - a_img.min()) / (a_img.max() - a_img.min() + 1e-8)\n",
                "#             # upsample to original patch size\n",
                "#             a_img_t = torch.from_numpy(a_img)[None,None]\n",
                "#             a_img_t = F.interpolate(a_img_t, size=(H0, W0), mode='bilinear', align_corners=False)\n",
                "#             attn_map_up = a_img_t[0,0].cpu().numpy()\n",
                "\n",
                "#         # 4) upsample prob map to original patch size\n",
                "#         prob_t = torch.from_numpy(prob_map)[None,None]\n",
                "#         prob_up = F.interpolate(prob_t, size=(H0, W0), mode='bilinear', align_corners=False)[0,0].cpu().numpy()\n",
                "\n",
                "#         # 5) per-instance stats and pooled features at model H,W, then map centroids to original coords\n",
                "#         nuclei = []\n",
                "#         scale_y = H0 / inst_map.shape[0]\n",
                "#         scale_x = W0 / inst_map.shape[1]\n",
                "#         types_map = assign_types_per_instance(type_logits, inst_map)\n",
                "#         pooled = pool_features_over_instances(feat_map if feat_map is not None else np.zeros((1,)+inst_map.shape), inst_map)\n",
                "\n",
                "#         for r in regionprops(inst_map):\n",
                "#             y, x = r.centroid\n",
                "#             cx, cy = float(x*scale_x), float(y*scale_y)\n",
                "#             k = int(r.label)\n",
                "#             item = {\n",
                "#                 \"inst_id\": k,\n",
                "#                 \"centroid_xy\": (cx, cy),               # in original patch coords\n",
                "#                 \"bbox_modelhw\": tuple(r.bbox),         # at model resolution\n",
                "#                 \"area_modelpx\": int(r.area),\n",
                "#                 \"class_id\": types_map.get(k, None),\n",
                "#                 \"feat\": pooled.get(k, None)            # np.ndarray or None\n",
                "#             }\n",
                "#             nuclei.append(item)\n",
                "\n",
                "#         # optional overlay\n",
                "#         overlay = None\n",
                "#         if return_overlay:\n",
                "#             overlay = self._make_overlay(np.array(pil_patch), inst_map, scale_x, scale_y)\n",
                "\n",
                "#         return nuclei, attn_map_up, prob_up, overlay\n",
                "\n",
                "#     @staticmethod\n",
                "#     def _make_overlay(rgb_uint8, inst_map, sx, sy):\n",
                "#         H0, W0 = rgb_uint8.shape[:2]\n",
                "#         fig, ax = plt.subplots(figsize=(5,5), dpi=120)\n",
                "#         ax.imshow(rgb_uint8)\n",
                "#         # draw contours for each instance after scaling\n",
                "#         for k in np.unique(inst_map)[1:]:\n",
                "#             m = (inst_map == k)\n",
                "#             contours = find_contours(m.astype(float), 0.5)\n",
                "#             for c in contours:\n",
                "#                 c[:, 0] *= sy\n",
                "#                 c[:, 1] *= sx\n",
                "#                 ax.plot(c[:,1], c[:,0])\n",
                "#         ax.set_axis_off()\n",
                "#         fig.tight_layout()\n",
                "#         return fig\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42ebc75f",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 12 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "33393aac",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "ename": "AttributeError",
                    "evalue": "'ViTFeatureExtractor' object has no attribute 'infer_on_patch'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m patch \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatches/Benign/b001_p0.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 3) run inference\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m nuclei, attn_map, prob_map, overlay_fig \u001b[38;5;241m=\u001b[39m \u001b[43mseg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_on_patch\u001b[49m(patch, return_overlay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 4) visualize: segmentation overlay\u001b[39;00m\n\u001b[1;32m     13\u001b[0m display(overlay_fig)\n",
                        "File \u001b[0;32m~/miniconda3/envs/clam_latest/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'ViTFeatureExtractor' object has no attribute 'infer_on_patch'"
                    ]
                }
            ],
            "source": [
                "# Cell 4 — example usage\n",
                "# 1) set your checkpoint path and construct the model\n",
                "weights_path = \"CellViT-256-x40.pth\"  # <<< change\n",
                "# seg = CellViTSegmentor(weights_path=weights_path, device='cuda', in_size=256, patch_size=16)\n",
                "seg = ViTFeatureExtractor()\n",
                "# 2) load a patch image (PIL). Use your own patch here.\n",
                "patch = Image.open(\"Patches/Benign/b001_p0.png\").convert(\"RGB\")\n",
                "\n",
                "# 3) run inference\n",
                "nuclei, attn_map, prob_map, overlay_fig = seg.infer_on_patch(patch, return_overlay=True)\n",
                "\n",
                "# 4) visualize: segmentation overlay\n",
                "display(overlay_fig)\n",
                "plt.close(overlay_fig)\n",
                "\n",
                "# 5) visualize attention heatmap\n",
                "if attn_map is not None:\n",
                "    fig2, ax2 = plt.subplots(figsize=(5,5), dpi=120)\n",
                "    ax2.imshow(patch)\n",
                "    ax2.imshow(attn_map, alpha=0.4)  # default colormap\n",
                "    ax2.set_axis_off()\n",
                "    fig2.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# 6) print a few nuclei records\n",
                "print(f\"Detected nuclei: {len(nuclei)}\")\n",
                "for rec in nuclei[:5]:\n",
                "    print({\n",
                "        \"inst_id\": rec[\"inst_id\"],\n",
                "        \"centroid_xy\": tuple(round(v,2) for v in rec[\"centroid_xy\"]),\n",
                "        \"area_modelpx\": rec[\"area_modelpx\"],\n",
                "        \"class_id\": rec[\"class_id\"],\n",
                "        \"feat_shape\": None if rec[\"feat\"] is None else tuple(rec[\"feat\"].shape)\n",
                "    })\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "41921a17",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 13 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a662985",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "938d36ed",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 14 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd91f28d",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "cfadadd3",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 15 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dbb57e63",
            "metadata": {},
            "source": [
                "## 3. Complete Graph Construction with Euclidean Distance"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b1f8070",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 16 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d492f755",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_complete_graph(coordinates, epsilon=1e-6):\n",
                "    \"\"\"\n",
                "    Build complete graph with edge weights = 1 / Euclidean distance.\n",
                "    \n",
                "    Args:\n",
                "        coordinates: (N, 2) array of (x, y) coordinates\n",
                "        epsilon: Small constant to avoid division by zero\n",
                "    \n",
                "    Returns:\n",
                "        edge_index: (2, E) tensor of edges (fully connected)\n",
                "        edge_weights: (E,) tensor of edge weights (inverse distance)\n",
                "    \"\"\"\n",
                "    N = len(coordinates)\n",
                "    \n",
                "    # Create all pairwise edges (complete graph)\n",
                "    sources = []\n",
                "    targets = []\n",
                "    distances = []\n",
                "    \n",
                "    for i in range(N):\n",
                "        for j in range(N):\n",
                "            if i != j:  # No self-loops\n",
                "                sources.append(i)\n",
                "                targets.append(j)\n",
                "                \n",
                "                # Euclidean distance\n",
                "                dist = np.sqrt(np.sum((coordinates[i] - coordinates[j])**2))\n",
                "                distances.append(dist)\n",
                "    \n",
                "    edge_index = torch.tensor([sources, targets], dtype=torch.long)\n",
                "    \n",
                "    # Edge weights = 1 / distance (inverse distance weighting)\n",
                "    distances = np.array(distances)\n",
                "    edge_weights = 1.0 / (distances + epsilon)\n",
                "    edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
                "    \n",
                "    # Normalize edge weights\n",
                "    edge_weights = edge_weights / edge_weights.max()\n",
                "    \n",
                "    return edge_index, edge_weights\n",
                "\n",
                "print(\"Complete graph construction defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5af4e8f7",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 17 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e20a4a3d",
            "metadata": {},
            "source": [
                "## 4. Graph Attention Layer with Edge Pruning"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "23c41223",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 18 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "08c1d1d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "class GraphAttentionLayer(nn.Module):\n",
                "    \"\"\"\n",
                "    Graph Attention Layer with:\n",
                "    - Edge attention scores\n",
                "    - Node attention scores (average of connected edges)\n",
                "    - Dynamic edge pruning based on threshold\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2, concat=True):\n",
                "        super().__init__()\n",
                "        self.in_features = in_features\n",
                "        self.out_features = out_features\n",
                "        self.dropout = dropout\n",
                "        self.alpha = alpha\n",
                "        self.concat = concat\n",
                "        \n",
                "        # Learnable transformation\n",
                "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
                "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
                "        \n",
                "        # Attention mechanism\n",
                "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
                "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
                "        \n",
                "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
                "    \n",
                "    def forward(self, h, edge_index, edge_weights, attention_threshold=0.0):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            h: (N, in_features) node features\n",
                "            edge_index: (2, E) edge connections\n",
                "            edge_weights: (E,) spatial edge weights\n",
                "            attention_threshold: Threshold for edge pruning (0-1)\n",
                "        \n",
                "        Returns:\n",
                "            h_prime: (N, out_features) transformed node features\n",
                "            edge_attention: (E,) attention scores for edges\n",
                "            node_attention: (N,) attention scores for nodes\n",
                "            pruned_edge_mask: (E,) boolean mask of kept edges\n",
                "        \"\"\"\n",
                "        # Linear transformation\n",
                "        Wh = torch.mm(h, self.W)  # (N, out_features)\n",
                "        \n",
                "        # Compute edge attention scores\n",
                "        edge_attention = self._compute_edge_attention(Wh, edge_index, edge_weights)\n",
                "        \n",
                "        # Apply attention threshold for edge pruning\n",
                "        pruned_edge_mask = edge_attention > attention_threshold\n",
                "        \n",
                "        # Prune edges below threshold\n",
                "        if pruned_edge_mask.sum() > 0:\n",
                "            edge_index_pruned = edge_index[:, pruned_edge_mask]\n",
                "            edge_attention_pruned = edge_attention[pruned_edge_mask]\n",
                "        else:\n",
                "            # Keep at least some edges\n",
                "            top_k = max(1, int(0.1 * len(edge_attention)))\n",
                "            _, top_indices = torch.topk(edge_attention, top_k)\n",
                "            edge_index_pruned = edge_index[:, top_indices]\n",
                "            edge_attention_pruned = edge_attention[top_indices]\n",
                "            pruned_edge_mask = torch.zeros_like(edge_attention, dtype=torch.bool)\n",
                "            pruned_edge_mask[top_indices] = True\n",
                "        \n",
                "        # Apply softmax per source node\n",
                "        edge_attention_normalized = self._normalize_attention(edge_attention_pruned, edge_index_pruned)\n",
                "        \n",
                "        # Apply dropout\n",
                "        edge_attention_normalized = F.dropout(edge_attention_normalized, self.dropout, training=self.training)\n",
                "        \n",
                "        # Aggregate features\n",
                "        h_prime = self._aggregate(Wh, edge_index_pruned, edge_attention_normalized)\n",
                "        \n",
                "        # Compute node attention scores (average of connected edge attentions)\n",
                "        node_attention = self._compute_node_attention(edge_attention, edge_index, h.size(0))\n",
                "        \n",
                "        if self.concat:\n",
                "            return F.elu(h_prime), edge_attention, node_attention, pruned_edge_mask\n",
                "        else:\n",
                "            return h_prime, edge_attention, node_attention, pruned_edge_mask\n",
                "    \n",
                "    def _compute_edge_attention(self, Wh, edge_index, edge_weights):\n",
                "        \"\"\"Compute attention coefficients for each edge.\"\"\"\n",
                "        source, target = edge_index[0], edge_index[1]\n",
                "        \n",
                "        # Concatenate source and target features\n",
                "        edge_features = torch.cat([Wh[source], Wh[target]], dim=1)  # (E, 2*out_features)\n",
                "        \n",
                "        # Compute attention logits\n",
                "        e = self.leakyrelu(torch.matmul(edge_features, self.a).squeeze(-1))  # (E,)\n",
                "        \n",
                "        # Incorporate spatial edge weights\n",
                "        e = e * edge_weights.to(e.device)\n",
                "        \n",
                "        return e\n",
                "    \n",
                "    def _normalize_attention(self, attention, edge_index):\n",
                "        \"\"\"Apply softmax normalization per source node.\"\"\"\n",
                "        source = edge_index[0]\n",
                "        \n",
                "        # Compute max per source for numerical stability\n",
                "        max_attention = torch.zeros(source.max() + 1, device=attention.device)\n",
                "        max_attention.index_reduce_(0, source, attention, 'amax', include_self=False)\n",
                "        \n",
                "        # Subtract max and exponentiate\n",
                "        attention_shifted = attention - max_attention[source]\n",
                "        attention_exp = torch.exp(attention_shifted)\n",
                "        \n",
                "        # Sum per source\n",
                "        attention_sum = torch.zeros(source.max() + 1, device=attention.device)\n",
                "        attention_sum.index_add_(0, source, attention_exp)\n",
                "        \n",
                "        # Normalize\n",
                "        attention_normalized = attention_exp / (attention_sum[source] + 1e-16)\n",
                "        \n",
                "        return attention_normalized\n",
                "    \n",
                "    def _aggregate(self, Wh, edge_index, attention):\n",
                "        \"\"\"Aggregate neighbor features weighted by attention.\"\"\"\n",
                "        source, target = edge_index[0], edge_index[1]\n",
                "        \n",
                "        # Weighted features\n",
                "        weighted_features = Wh[target] * attention.unsqueeze(-1)\n",
                "        \n",
                "        # Sum aggregation per node\n",
                "        h_prime = torch.zeros(Wh.size(0), Wh.size(1), device=Wh.device)\n",
                "        h_prime.index_add_(0, source, weighted_features)\n",
                "        \n",
                "        return h_prime\n",
                "    \n",
                "    def _compute_node_attention(self, edge_attention, edge_index, num_nodes):\n",
                "        \"\"\"Compute node attention as average of connected edge attentions.\"\"\"\n",
                "        source = edge_index[0]\n",
                "        \n",
                "        # Sum of edge attentions per node\n",
                "        node_attention_sum = torch.zeros(num_nodes, device=edge_attention.device)\n",
                "        node_attention_sum.index_add_(0, source, edge_attention)\n",
                "        \n",
                "        # Count edges per node\n",
                "        node_degree = torch.zeros(num_nodes, device=edge_attention.device)\n",
                "        node_degree.index_add_(0, source, torch.ones_like(edge_attention))\n",
                "        \n",
                "        # Average attention\n",
                "        node_attention = node_attention_sum / (node_degree + 1e-16)\n",
                "        \n",
                "        return node_attention\n",
                "\n",
                "print(\"Graph Attention Layer with edge pruning defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4843148d",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 19 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "346f96bd",
            "metadata": {},
            "source": [
                "## 5. Multi-layer GAT Model with Classification"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e0ee48bf",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 20 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e5ea34b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "class GATClassifier(nn.Module):\n",
                "    \"\"\"\n",
                "    Graph Attention Network for Histopathology Classification.\n",
                "    \n",
                "    Architecture:\n",
                "    - Input: Node features (768-D from ViT)\n",
                "    - GAT Layer 1: 768 → 256 (multi-head attention)\n",
                "    - GAT Layer 2: 256 → 128\n",
                "    - Global pooling: Attention-weighted graph representation\n",
                "    - Classifier: 128 → num_classes\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, feature_extractor, num_classes=4, hidden_dim=256, \n",
                "                 num_heads=4, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.feature_extractor = feature_extractor\n",
                "        self.num_classes = num_classes\n",
                "        self.num_heads = num_heads\n",
                "        \n",
                "        # Multi-head GAT layers\n",
                "        self.gat1_heads = nn.ModuleList([\n",
                "            GraphAttentionLayer(768, hidden_dim, dropout=dropout, concat=True)\n",
                "            for _ in range(num_heads)\n",
                "        ])\n",
                "        \n",
                "        self.gat2_heads = nn.ModuleList([\n",
                "            GraphAttentionLayer(hidden_dim * num_heads, hidden_dim // 2, \n",
                "                              dropout=dropout, concat=True)\n",
                "            for _ in range(num_heads)\n",
                "        ])\n",
                "        \n",
                "        # Final feature dimension after concatenating heads\n",
                "        final_dim = (hidden_dim // 2) * num_heads\n",
                "        \n",
                "        # Global attention pooling\n",
                "        self.global_attention = nn.Sequential(\n",
                "            nn.Linear(final_dim, hidden_dim // 2),\n",
                "            nn.Tanh(),\n",
                "            nn.Linear(hidden_dim // 2, 1)\n",
                "        )\n",
                "        \n",
                "        # Classifier\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(final_dim, hidden_dim // 2),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim // 2, num_classes)\n",
                "        )\n",
                "        \n",
                "        # Attention threshold (learnable parameter)\n",
                "        self.attention_threshold = nn.Parameter(torch.tensor(0.1))\n",
                "    \n",
                "    def forward(self, images, coordinates, return_attention=False):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            images: List of PIL images (patches)\n",
                "            coordinates: (N, 2) numpy array of patch coordinates\n",
                "            return_attention: Whether to return attention maps\n",
                "        \n",
                "        Returns:\n",
                "            logits: (num_classes,) classification logits\n",
                "            attention_info: Dict with attention scores (if return_attention=True)\n",
                "        \"\"\"\n",
                "        # Extract features\n",
                "        node_features = self.feature_extractor(images)  # (N, 768)\n",
                "        \n",
                "        # Build complete graph\n",
                "        edge_index, edge_weights = build_complete_graph(coordinates)\n",
                "        edge_index = edge_index.to(node_features.device)\n",
                "        edge_weights = edge_weights.to(node_features.device)\n",
                "        \n",
                "        # Apply attention threshold (sigmoid to keep in [0, 1])\n",
                "        threshold = torch.sigmoid(self.attention_threshold)\n",
                "        \n",
                "        # GAT Layer 1 (multi-head)\n",
                "        gat1_outputs = []\n",
                "        edge_attentions_1 = []\n",
                "        node_attentions_1 = []\n",
                "        edge_masks_1 = []\n",
                "        \n",
                "        for head in self.gat1_heads:\n",
                "            h, edge_att, node_att, edge_mask = head(\n",
                "                node_features, edge_index, edge_weights, threshold\n",
                "            )\n",
                "            gat1_outputs.append(h)\n",
                "            edge_attentions_1.append(edge_att)\n",
                "            node_attentions_1.append(node_att)\n",
                "            edge_masks_1.append(edge_mask)\n",
                "        \n",
                "        # Concatenate multi-head outputs\n",
                "        h1 = torch.cat(gat1_outputs, dim=1)  # (N, hidden_dim * num_heads)\n",
                "        \n",
                "        # GAT Layer 2 (multi-head)\n",
                "        gat2_outputs = []\n",
                "        edge_attentions_2 = []\n",
                "        node_attentions_2 = []\n",
                "        edge_masks_2 = []\n",
                "        \n",
                "        for head in self.gat2_heads:\n",
                "            h, edge_att, node_att, edge_mask = head(\n",
                "                h1, edge_index, edge_weights, threshold\n",
                "            )\n",
                "            gat2_outputs.append(h)\n",
                "            edge_attentions_2.append(edge_att)\n",
                "            node_attentions_2.append(node_att)\n",
                "            edge_masks_2.append(edge_mask)\n",
                "        \n",
                "        # Concatenate multi-head outputs\n",
                "        h2 = torch.cat(gat2_outputs, dim=1)  # (N, final_dim)\n",
                "        \n",
                "        # Global attention pooling\n",
                "        global_att_scores = self.global_attention(h2)  # (N, 1)\n",
                "        global_att_weights = F.softmax(global_att_scores, dim=0)  # (N, 1)\n",
                "        \n",
                "        # Graph-level representation\n",
                "        graph_features = torch.sum(global_att_weights * h2, dim=0)  # (final_dim,)\n",
                "        \n",
                "        # Classification\n",
                "        logits = self.classifier(graph_features)  # (num_classes,)\n",
                "        \n",
                "        if return_attention:\n",
                "            # Average node attention across heads\n",
                "            node_attention_avg = torch.stack(node_attentions_2).mean(dim=0)\n",
                "            \n",
                "            attention_info = {\n",
                "                'node_attention': node_attention_avg.detach().cpu().numpy(),\n",
                "                'global_attention': global_att_weights.squeeze().detach().cpu().numpy(),\n",
                "                'edge_attentions_layer1': [e.detach().cpu().numpy() for e in edge_attentions_1],\n",
                "                'edge_attentions_layer2': [e.detach().cpu().numpy() for e in edge_attentions_2],\n",
                "                'edge_masks_layer1': [m.detach().cpu().numpy() for m in edge_masks_1],\n",
                "                'edge_masks_layer2': [m.detach().cpu().numpy() for m in edge_masks_2],\n",
                "                'threshold': threshold.item(),\n",
                "                'num_edges_kept': [m.sum().item() for m in edge_masks_2]\n",
                "            }\n",
                "            return logits, attention_info\n",
                "        \n",
                "        return logits\n",
                "\n",
                "print(\"GAT Classifier model defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9465a695",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 21 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5d3e36c1",
            "metadata": {},
            "source": [
                "## 6. Dataset and DataLoader"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "59c4088c",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 22 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2a27615",
            "metadata": {},
            "outputs": [],
            "source": [
                "class HistoGraphDataset(Dataset):\n",
                "    \"\"\"Dataset for histopathology images as graphs.\"\"\"\n",
                "    \n",
                "    def __init__(self, root_dir, class_names=['Benign', 'InSitu', 'Invasive', 'Normal']):\n",
                "        self.root_dir = root_dir\n",
                "        self.class_names = class_names\n",
                "        self.class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
                "        \n",
                "        # Collect all image paths\n",
                "        self.samples = []\n",
                "        for class_name in class_names:\n",
                "            class_dir = os.path.join(root_dir, class_name)\n",
                "            if os.path.exists(class_dir):\n",
                "                for img_name in os.listdir(class_dir):\n",
                "                    if img_name.endswith(('.tif', '.png', '.jpg', '.jpeg')):\n",
                "                        img_path = os.path.join(class_dir, img_name)\n",
                "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
                "        \n",
                "        print(f\"Found {len(self.samples)} images across {len(class_names)} classes.\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_path, label = self.samples[idx]\n",
                "        \n",
                "        # Extract patches with coordinates\n",
                "        patches, coordinates = extract_patches_with_coords(img_path)\n",
                "        \n",
                "        return patches, coordinates, label, img_path\n",
                "\n",
                "print(\"Dataset class defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6c54c1dd",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 23 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "46db102a",
            "metadata": {},
            "source": [
                "## 7. Training Configuration"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3e18193e",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 24 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "04233d7c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "NUM_CLASSES = 4\n",
                "HIDDEN_DIM = 256\n",
                "NUM_HEADS = 4\n",
                "DROPOUT = 0.3\n",
                "LEARNING_RATE = 1e-4\n",
                "NUM_EPOCHS = 15\n",
                "BATCH_SIZE = 1  # Process one image at a time (variable number of patches)\n",
                "\n",
                "# Dataset paths\n",
                "TRAIN_DIR = '/home/pclab/Desktop/WORK/histoAttribution/Photos'\n",
                "CLASS_NAMES = ['Benign', 'InSitu', 'Invasive', 'Normal']\n",
                "\n",
                "print(\"Configuration set.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa0e0be3",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 25 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "78aa5f38",
            "metadata": {},
            "source": [
                "## 8. Initialize Model and Dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ccca77c0",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 26 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dc03a565",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize feature extractor\n",
                "print(\"Loading ViT feature extractor...\")\n",
                "feature_extractor = ViTFeatureExtractor().to(device)\n",
                "\n",
                "# Initialize GAT model\n",
                "print(\"Initializing GAT Classifier...\")\n",
                "model = GATClassifier(\n",
                "    feature_extractor=feature_extractor,\n",
                "    num_classes=NUM_CLASSES,\n",
                "    hidden_dim=HIDDEN_DIM,\n",
                "    num_heads=NUM_HEADS,\n",
                "    dropout=DROPOUT\n",
                ").to(device)\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"Total parameters: {total_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")\n",
                "\n",
                "# Load dataset\n",
                "print(\"\\nLoading dataset...\")\n",
                "dataset = HistoGraphDataset(TRAIN_DIR, CLASS_NAMES)\n",
                "\n",
                "# Split into train/val\n",
                "train_size = int(0.8 * len(dataset))\n",
                "val_size = len(dataset) - train_size\n",
                "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
                "\n",
                "print(f\"Train samples: {len(train_dataset)}\")\n",
                "print(f\"Validation samples: {len(val_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d71f3f2c",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 27 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0bda9d7",
            "metadata": {},
            "source": [
                "## 9. Training Loop"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25a2752e",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 28 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e3263db6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loss and optimizer\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(\n",
                "    filter(lambda p: p.requires_grad, model.parameters()),\n",
                "    lr=LEARNING_RATE\n",
                ")\n",
                "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
                "    optimizer, mode='max', factor=0.5, patience=3\n",
                ")\n",
                "\n",
                "# Training history\n",
                "history = {\n",
                "    'train_loss': [], 'train_acc': [],\n",
                "    'val_loss': [], 'val_acc': [],\n",
                "    'threshold': [], 'avg_edges_kept': []\n",
                "}\n",
                "\n",
                "print(\"Starting training...\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "960a6725",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 29 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6cdc1f6a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, dataset, criterion, optimizer, device):\n",
                "    \"\"\"Train for one epoch.\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for patches, coordinates, label, _ in tqdm(dataset, desc=\"Training\"):\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward pass\n",
                "        logits = model(patches, coordinates)\n",
                "        \n",
                "        # Compute loss\n",
                "        label = torch.tensor([label], dtype=torch.long).to(device)\n",
                "        loss = criterion(logits.unsqueeze(0), label)\n",
                "        \n",
                "        # Backward pass\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        # Metrics\n",
                "        total_loss += loss.item()\n",
                "        pred = torch.argmax(logits)\n",
                "        correct += (pred == label).sum().item()\n",
                "        total += 1\n",
                "    \n",
                "    avg_loss = total_loss / total\n",
                "    accuracy = correct / total\n",
                "    \n",
                "    return avg_loss, accuracy\n",
                "\n",
                "def validate_epoch(model, dataset, criterion, device):\n",
                "    \"\"\"Validate for one epoch.\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    total_edges_kept = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for patches, coordinates, label, _ in tqdm(dataset, desc=\"Validation\"):\n",
                "            # Forward pass\n",
                "            logits, attention_info = model(patches, coordinates, return_attention=True)\n",
                "            \n",
                "            # Compute loss\n",
                "            label_tensor = torch.tensor([label], dtype=torch.long).to(device)\n",
                "            loss = criterion(logits.unsqueeze(0), label_tensor)\n",
                "            \n",
                "            # Metrics\n",
                "            total_loss += loss.item()\n",
                "            pred = torch.argmax(logits).item()\n",
                "            correct += (pred == label)\n",
                "            total += 1\n",
                "            \n",
                "            all_preds.append(pred)\n",
                "            all_labels.append(label)\n",
                "            \n",
                "            # Track edges kept\n",
                "            total_edges_kept += np.mean(attention_info['num_edges_kept'])\n",
                "    \n",
                "    avg_loss = total_loss / total\n",
                "    accuracy = correct / total\n",
                "    avg_edges_kept = total_edges_kept / total\n",
                "    \n",
                "    return avg_loss, accuracy, all_preds, all_labels, avg_edges_kept\n",
                "\n",
                "print(\"Training and validation functions defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9e88ae7a",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 30 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d701ff8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "\n",
                "# Ensure checkpoint dir\n",
                "os.makedirs('checkpoints', exist_ok=True)\n",
                "checkpoint_path = 'checkpoints/GAT_best.pth'\n",
                "\n",
                "# Optionally resume\n",
                "best_val_acc = 0.0\n",
                "if os.path.exists(checkpoint_path):\n",
                "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
                "    model.load_state_dict(ckpt['model_state_dict'])\n",
                "    if 'optimizer_state_dict' in ckpt:\n",
                "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
                "    print(f\"Loaded checkpoint: {checkpoint_path}\")\n",
                "    print(f\"Stored Best Val Acc: {ckpt.get('val_acc', 0.0):.4f}\")\n",
                "    # Re-evaluate current validation accuracy\n",
                "    val_loss0, val_acc0, _, _, avg_edges0 = validate_epoch(model, val_dataset, criterion, device)\n",
                "    print(f\"Re-evaluated Val Loss: {val_loss0:.4f} | Val Acc: {val_acc0:.4f}\")\n",
                "    print(f\"Attention Threshold: {torch.sigmoid(model.attention_threshold).item():.4f}\")\n",
                "    print(f\"Average Edges Kept: {avg_edges0:.1f}\")\n",
                "    best_val_acc = max(ckpt.get('val_acc', 0.0), val_acc0)\n",
                "else:\n",
                "    print(\"No previous checkpoint found. Starting from scratch.\")\n",
                "\n",
                "    # Training loop\n",
                "    for epoch in range(NUM_EPOCHS):\n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
                "        print(f\"{'='*60}\")\n",
                "        \n",
                "        # Train\n",
                "        train_loss, train_acc = train_epoch(model, train_dataset, criterion, optimizer, device)\n",
                "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
                "        \n",
                "        # Validate\n",
                "        val_loss, val_acc, val_preds, val_labels, avg_edges = validate_epoch(\n",
                "            model, val_dataset, criterion, device\n",
                "        )\n",
                "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
                "        print(f\"Attention Threshold: {torch.sigmoid(model.attention_threshold).item():.4f}\")\n",
                "        print(f\"Average Edges Kept: {avg_edges:.1f}\")\n",
                "        \n",
                "        # Update learning rate\n",
                "        scheduler.step(val_acc)\n",
                "        \n",
                "        # Save history\n",
                "        history['train_loss'].append(train_loss)\n",
                "        history['train_acc'].append(train_acc)\n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['val_acc'].append(val_acc)\n",
                "        history['threshold'].append(torch.sigmoid(model.attention_threshold).item())\n",
                "        history['avg_edges_kept'].append(avg_edges)\n",
                "        \n",
                "        # Save best model\n",
                "        if val_acc > best_val_acc:\n",
                "            best_val_acc = val_acc\n",
                "            torch.save({\n",
                "                'epoch': epoch,\n",
                "                'model_state_dict': model.state_dict(),\n",
                "                'optimizer_state_dict': optimizer.state_dict(),\n",
                "                'val_acc': val_acc,\n",
                "                'history': history\n",
                "            }, checkpoint_path)\n",
                "            print(f\"✓ Best model saved! (Val Acc: {val_acc:.4f})\")\n",
                "\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(f\"Training completed! Best Val Acc: {best_val_acc:.4f}\")\n",
                "    print(\"=\"*60)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2def431e",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 31 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "22903c67",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "b348764e",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 32 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6a65794",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "873b087e",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 33 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d19ad5cb",
            "metadata": {},
            "source": [
                "## 10. Training Visualization"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "818649df",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 34 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "90dca8ec",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== Plot using history loaded from checkpoint (standalone) =====\n",
                "import os, torch\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "checkpoint_path = 'checkpoints/GAT_best.pth'\n",
                "\n",
                "def load_history(path):\n",
                "    if not os.path.exists(path):\n",
                "        raise FileNotFoundError(f\"No checkpoint at {path}\")\n",
                "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
                "    hist = ckpt.get('history', {})\n",
                "    # ensure all keys exist\n",
                "    for k in ['train_loss','train_acc','val_loss','val_acc','threshold','avg_edges_kept']:\n",
                "        hist.setdefault(k, [])\n",
                "    return hist\n",
                "\n",
                "# prefer in-memory `history` if present and non-empty, else load from disk\n",
                "try:\n",
                "    # If history exists and has data, use it; otherwise load from checkpoint\n",
                "    if 'history' in locals() and sum(len(v) for v in history.values()) > 0:\n",
                "        use_hist = history\n",
                "    else:\n",
                "        use_hist = load_history(checkpoint_path)\n",
                "except NameError:\n",
                "    use_hist = load_history(checkpoint_path)\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "axes[0, 0].plot(use_hist['train_loss'], label='Train Loss', marker='o')\n",
                "axes[0, 0].plot(use_hist['val_loss'], label='Val Loss', marker='s')\n",
                "axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Loss'); axes[0, 0].set_title('Training and Validation Loss')\n",
                "axes[0, 0].legend(); axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[0, 1].plot(use_hist['train_acc'], label='Train Acc', marker='o')\n",
                "axes[0, 1].plot(use_hist['val_acc'], label='Val Acc', marker='s')\n",
                "axes[0, 1].set_xlabel('Epoch'); axes[0, 1].set_ylabel('Accuracy'); axes[0, 1].set_title('Training and Validation Accuracy')\n",
                "axes[0, 1].legend(); axes[0, 1].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1, 0].plot(use_hist['threshold'], marker='o')\n",
                "axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('Threshold Value'); axes[1, 0].set_title('Attention Threshold Evolution')\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1, 1].plot(use_hist['avg_edges_kept'], marker='o')\n",
                "axes[1, 1].set_xlabel('Epoch'); axes[1, 1].set_ylabel('Number of Edges'); axes[1, 1].set_title('Average Edges Kept After Pruning')\n",
                "axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('plots/GAT_training_curves.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"Training curves saved to plots/GAT_training_curves.png\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fe5c4489",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 35 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83fb8964",
            "metadata": {},
            "source": [
                "## 11. Confusion Matrix and Classification Report"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "930315f1",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 36 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6474d9c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model\n",
                "checkpoint = torch.load('checkpoints/GAT_best.pth', map_location=device, weights_only=False)\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
                "\n",
                "# Get predictions on validation set\n",
                "model.eval()\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for patches, coordinates, label, _ in tqdm(val_dataset, desc=\"Final Evaluation\"):\n",
                "        logits = model(patches, coordinates)\n",
                "        pred = torch.argmax(logits).item()\n",
                "        all_preds.append(pred)\n",
                "        all_labels.append(label)\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(all_labels, all_preds)\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
                "plt.title('GAT Confusion Matrix (Validation Set)')\n",
                "plt.ylabel('True Label')\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.savefig('plots/GAT_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n",
                "\n",
                "print(f\"\\nFinal Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "721b918d",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 37 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "afb09d57",
            "metadata": {},
            "source": [
                "## 12. Attention Visualization"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "80917d6a",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 38 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ca75f55",
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_graph_attention(model, image_path, class_name, device):\n",
                "    \"\"\"\n",
                "    Visualize graph structure and attention for a sample image.\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Extract patches\n",
                "    patches, coordinates = extract_patches_with_coords(image_path)\n",
                "    \n",
                "    # Get predictions and attention\n",
                "    with torch.no_grad():\n",
                "        logits, attention_info = model(patches, coordinates, return_attention=True)\n",
                "    \n",
                "    pred_class = torch.argmax(logits).item()\n",
                "    confidence = F.softmax(logits, dim=0)[pred_class].item()\n",
                "    \n",
                "    # Create visualization\n",
                "    fig = plt.figure(figsize=(20, 10))\n",
                "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
                "    \n",
                "    # Original image\n",
                "    ax1 = fig.add_subplot(gs[0, 0])\n",
                "    img = Image.open(image_path).convert('RGB')\n",
                "    ax1.imshow(img)\n",
                "    ax1.set_title(f'Original Image\\nTrue: {class_name} | Pred: {CLASS_NAMES[pred_class]} ({confidence:.2f})')\n",
                "    ax1.axis('off')\n",
                "    \n",
                "    # Node attention heatmap\n",
                "    ax2 = fig.add_subplot(gs[0, 1])\n",
                "    node_attention = attention_info['node_attention']\n",
                "    scatter = ax2.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=node_attention, cmap='hot', s=100, alpha=0.7)\n",
                "    ax2.set_title('Node Attention Scores\\n(Avg of Edge Attentions)')\n",
                "    ax2.invert_yaxis()\n",
                "    plt.colorbar(scatter, ax=ax2)\n",
                "    \n",
                "    # Global attention heatmap\n",
                "    ax3 = fig.add_subplot(gs[0, 2])\n",
                "    global_attention = attention_info['global_attention']\n",
                "    scatter = ax3.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=global_attention, cmap='viridis', s=100, alpha=0.7)\n",
                "    ax3.set_title('Global Pooling Attention')\n",
                "    ax3.invert_yaxis()\n",
                "    plt.colorbar(scatter, ax=ax3)\n",
                "    \n",
                "    # Graph structure (pruned edges)\n",
                "    ax4 = fig.add_subplot(gs[1, :])\n",
                "    \n",
                "    # Build edge list from kept edges (using first head)\n",
                "    edge_index, _ = build_complete_graph(coordinates)\n",
                "    edge_mask = attention_info['edge_masks_layer2'][0]\n",
                "    kept_edges = edge_index[:, edge_mask]\n",
                "    \n",
                "    # Plot nodes\n",
                "    scatter = ax4.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=node_attention, cmap='hot', s=150, alpha=0.8, \n",
                "                         edgecolors='black', linewidths=2, zorder=3)\n",
                "    \n",
                "    # Plot kept edges\n",
                "    for i in range(kept_edges.shape[1]):\n",
                "        src, tgt = kept_edges[0, i], kept_edges[1, i]\n",
                "        ax4.plot([coordinates[src, 0], coordinates[tgt, 0]], \n",
                "                [coordinates[src, 1], coordinates[tgt, 1]], \n",
                "                'gray', alpha=0.9, linewidth=1, zorder=1)\n",
                "    \n",
                "    ax4.set_title(f'Graph Structure (Pruned)\\nThreshold: {attention_info[\"threshold\"]:.3f} | '\n",
                "                 f'Edges Kept: {np.mean(attention_info[\"num_edges_kept\"]):.0f}')\n",
                "    ax4.invert_yaxis()\n",
                "    plt.colorbar(scatter, ax=ax4, label='Node Attention')\n",
                "    \n",
                "    plt.savefig(f'attention_viz_GAT_{class_name}.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nAttention Statistics:\")\n",
                "    print(f\"  Node attention range: [{node_attention.min():.4f}, {node_attention.max():.4f}]\")\n",
                "    print(f\"  Global attention range: [{global_attention.min():.4f}, {global_attention.max():.4f}]\")\n",
                "    print(f\"  Edges kept: {np.mean(attention_info['num_edges_kept']):.0f} / {len(edge_mask)}\")\n",
                "    print(f\"  Top-5 nodes by attention: {np.argsort(node_attention)[-5:][::-1]}\")\n",
                "\n",
                "print(\"Visualization function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3745b2d5",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 39 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "988f296b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize samples from each class\n",
                "for class_name in CLASS_NAMES:\n",
                "    class_dir = os.path.join(TRAIN_DIR, class_name)\n",
                "    if os.path.exists(class_dir):\n",
                "        images = [f for f in os.listdir(class_dir) if f.endswith(('.tif', '.png', '.jpg'))]\n",
                "        if images:\n",
                "            sample_image = os.path.join(class_dir, images[0])\n",
                "            print(f\"\\n{'='*60}\")\n",
                "            print(f\"Visualizing: {class_name}\")\n",
                "            print(f\"{'='*60}\")\n",
                "            visualize_graph_attention(model, sample_image, class_name, device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d9d21a09",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 40 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6e49c6cf",
            "metadata": {},
            "source": [
                "## 13. Save Final Results"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb797199",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 41 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ef29f4f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Save training history\n",
                "with open('checkpoints/GAT_history.json', 'w') as f:\n",
                "    json.dump(history, f, indent=4)\n",
                "\n",
                "print(\"Training history saved to checkpoints/GAT_history.json\")\n",
                "\n",
                "# Print final summary\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"FINAL RESULTS - GAT with Attention-based Edge Pruning\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
                "# print(f\"Final Attention Threshold: {history['threshold'][-1]:.4f}\")\n",
                "# print(f\"Final Edges Kept (avg): {history['avg_edges_kept'][-1]:.1f}\")\n",
                "# print(f\"\\nModel Architecture:\")\n",
                "print(f\"  - Input features: 768 (ViT)\")\n",
                "print(f\"  - Hidden dimension: {HIDDEN_DIM}\")\n",
                "print(f\"  - Number of heads: {NUM_HEADS}\")\n",
                "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "61006c2f",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 42 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "40b624fe",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Key Features Implemented:\n",
                "\n",
                "1. **Complete Graph Initialization**\n",
                "   - All patches (nodes) connected to each other\n",
                "   - Edge weights = 1 / Euclidean distance (spatial proximity)\n",
                "\n",
                "2. **Graph Attention Mechanism**\n",
                "   - Multi-head attention (4 heads)\n",
                "   - Edge-level attention scores\n",
                "   - Node attention = average of connected edge attentions\n",
                "\n",
                "3. **Dynamic Edge Pruning**\n",
                "   - Learnable attention threshold\n",
                "   - Edges below threshold are removed\n",
                "   - Graph becomes sparser during training\n",
                "\n",
                "4. **Two-layer GAT**\n",
                "   - Layer 1: 768 → 256 (feature transformation)\n",
                "   - Layer 2: 256 → 128 (refinement)\n",
                "   - Global attention pooling for classification\n",
                "\n",
                "5. **Interpretability**\n",
                "   - Visualize node attention scores\n",
                "   - Visualize graph structure evolution\n",
                "   - Track edge pruning over epochs\n",
                "\n",
                "### Advantages over MIL:\n",
                "- **Spatial relationships**: Edges encode patch proximity\n",
                "- **Message passing**: Information flows between neighboring patches\n",
                "- **Adaptive structure**: Graph topology learned during training\n",
                "- **Richer representations**: Multi-hop reasoning through graph layers"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7694d779",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 43 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6c3c2c45",
            "metadata": {},
            "source": [
                "## 14. ViT + GAT Attention Fusion for Enhanced Attribution\n",
                "\n",
                "This section extracts:\n",
                "- **ViT Internal Attention**: Per-patch attention weights from transformer blocks\n",
                "- **GAT Node Attention**: Graph-based spatial attention\n",
                "- **Fused Attention**: Combined multi-level attribution map"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "129e1036",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 44 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed7a5641",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ViTFeatureExtractorWithAttention(nn.Module):\n",
                "    \"\"\"\n",
                "    ViT Feature Extractor that also returns internal attention weights.\n",
                "    This allows us to see which parts of each patch the ViT focuses on.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
                "        super().__init__()\n",
                "        self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
                "        self.vit = ViTModel.from_pretrained(model_name, output_attentions=True)\n",
                "        \n",
                "        # Freeze all parameters\n",
                "        for param in self.vit.parameters():\n",
                "            param.requires_grad = False\n",
                "        \n",
                "        self.vit.eval()\n",
                "    \n",
                "    def forward(self, images, return_attention=False):\n",
                "        \"\"\"\n",
                "        Extract features and optionally attention weights.\n",
                "        \n",
                "        Returns:\n",
                "            features: (N, 768) CLS token features\n",
                "            attentions: List of (N, num_heads, num_patches, num_patches) attention matrices\n",
                "        \"\"\"\n",
                "        inputs = self.processor(images=images, return_tensors=\"pt\")\n",
                "        inputs = {k: v.to(next(self.vit.parameters()).device) for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.vit(**inputs)\n",
                "            features = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
                "            \n",
                "            if return_attention:\n",
                "                # Get attention from all layers\n",
                "                # Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
                "                attentions = outputs.attentions\n",
                "                return features, attentions\n",
                "            \n",
                "            return features\n",
                "    \n",
                "    def get_patch_attention_scores(self, images):\n",
                "        \"\"\"\n",
                "        Get aggregated attention scores for each whole-slide patch.\n",
                "        \n",
                "        Args:\n",
                "            images: List of PIL images (the whole-slide patches)\n",
                "        \n",
                "        Returns:\n",
                "            patch_attention: (N,) average attention score per whole-slide patch\n",
                "                            where N is the number of whole-slide patches\n",
                "        \"\"\"\n",
                "        _, attentions = self.forward(images, return_attention=True)\n",
                "        \n",
                "        # Aggregate attention across layers and heads for EACH whole-slide patch\n",
                "        # attentions is a tuple of (num_layers,) where each element has shape:\n",
                "        # (batch_size, num_heads, seq_len, seq_len)\n",
                "        \n",
                "        patch_attention_scores = []\n",
                "        \n",
                "        # Process each whole-slide patch (each element in the batch)\n",
                "        num_whole_patches = len(images)\n",
                "        \n",
                "        for patch_idx in range(num_whole_patches):\n",
                "            all_layer_attentions = []\n",
                "            \n",
                "            for layer_attention in attentions:\n",
                "                # layer_attention shape: (batch_size, num_heads, seq_len, seq_len)\n",
                "                # Get attention for this specific whole-slide patch\n",
                "                patch_att = layer_attention[patch_idx]  # (num_heads, seq_len, seq_len)\n",
                "                \n",
                "                # Average across heads\n",
                "                head_avg = patch_att.mean(dim=0)  # (seq_len, seq_len)\n",
                "                \n",
                "                # Get attention from CLS token (position 0) to all internal patches\n",
                "                # and sum/average to get overall importance of this whole-slide patch\n",
                "                cls_to_patches = head_avg[0, 1:]  # (num_internal_patches,)\n",
                "                \n",
                "                # Aggregate: mean attention from CLS to all internal patches\n",
                "                patch_importance = cls_to_patches.mean()\n",
                "                \n",
                "                all_layer_attentions.append(patch_importance)\n",
                "            \n",
                "            # Average across all layers for this whole-slide patch\n",
                "            avg_attention = torch.stack(all_layer_attentions).mean()\n",
                "            patch_attention_scores.append(avg_attention.item())\n",
                "        \n",
                "        return np.array(patch_attention_scores)\n",
                "\n",
                "print(\"ViT Feature Extractor with Attention defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b1537150",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 45 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2bafd41",
            "metadata": {},
            "outputs": [],
            "source": [
                "def fuse_vit_gat_attention(vit_attention, gat_attention, fusion_method='multiplicative', alpha=0.5):\n",
                "    \"\"\"\n",
                "    Fuse ViT internal attention with GAT node attention.\n",
                "    \n",
                "    Args:\n",
                "        vit_attention: (N,) ViT patch attention scores\n",
                "        gat_attention: (N,) GAT node attention scores\n",
                "        fusion_method: 'additive', 'multiplicative', or 'weighted'\n",
                "        alpha: Weight for additive/weighted fusion (0-1)\n",
                "    \n",
                "    Returns:\n",
                "        fused_attention: (N,) Combined attention scores\n",
                "    \"\"\"\n",
                "    # Normalize both attention maps to [0, 1]\n",
                "    vit_norm = (vit_attention - vit_attention.min()) / (vit_attention.max() - vit_attention.min() + 1e-8)\n",
                "    gat_norm = (gat_attention - gat_attention.min()) / (gat_attention.max() - gat_attention.min() + 1e-8)\n",
                "    \n",
                "    if fusion_method == 'additive':\n",
                "        # Weighted sum\n",
                "        fused = alpha * vit_norm + (1 - alpha) * gat_norm\n",
                "    \n",
                "    elif fusion_method == 'multiplicative':\n",
                "        # Element-wise multiplication (highlights regions with high attention in BOTH)\n",
                "        fused = vit_norm * gat_norm\n",
                "        # Normalize again\n",
                "        fused = (fused - fused.min()) / (fused.max() - fused.min() + 1e-8)\n",
                "    \n",
                "    elif fusion_method == 'weighted':\n",
                "        # Weighted geometric mean\n",
                "        fused = (vit_norm ** alpha) * (gat_norm ** (1 - alpha))\n",
                "    \n",
                "    else:\n",
                "        raise ValueError(f\"Unknown fusion method: {fusion_method}\")\n",
                "    \n",
                "    return fused\n",
                "\n",
                "print(\"Attention fusion function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37665928",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 46 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64dfb0f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_fused_attention(model, vit_with_attention, image_path, class_name, device, \n",
                "                              fusion_methods=['additive', 'multiplicative', 'weighted']):\n",
                "    \"\"\"\n",
                "    Extract and visualize ViT attention, GAT attention, and their fusion.\n",
                "    \n",
                "    This creates a comprehensive attribution map showing:\n",
                "    1. ViT's internal patch attention\n",
                "    2. GAT's graph-based node attention\n",
                "    3. Multiple fusion strategies\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    vit_with_attention.eval()\n",
                "    \n",
                "    # Load and extract patches\n",
                "    patches, coordinates = extract_patches_with_coords(image_path)\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Processing: {class_name}\")\n",
                "    print(f\"Number of patches: {len(patches)}\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    # ========================================\n",
                "    # 1. Get ViT Internal Attention\n",
                "    # ========================================\n",
                "    print(\"Extracting ViT internal attention...\")\n",
                "    vit_patch_attention = vit_with_attention.get_patch_attention_scores(patches)\n",
                "    print(f\"  ViT attention shape: {vit_patch_attention.shape}\")\n",
                "    print(f\"  ViT attention range: [{vit_patch_attention.min():.4f}, {vit_patch_attention.max():.4f}]\")\n",
                "    \n",
                "    # ========================================\n",
                "    # 2. Get GAT Node Attention\n",
                "    # ========================================\n",
                "    print(\"\\nExtracting GAT node attention...\")\n",
                "    with torch.no_grad():\n",
                "        logits, attention_info = model(patches, coordinates, return_attention=True)\n",
                "    \n",
                "    gat_node_attention = attention_info['node_attention']\n",
                "    gat_global_attention = attention_info['global_attention']\n",
                "    \n",
                "    print(f\"  GAT node attention shape: {gat_node_attention.shape}\")\n",
                "    print(f\"  GAT node attention range: [{gat_node_attention.min():.4f}, {gat_node_attention.max():.4f}]\")\n",
                "    \n",
                "    pred_class = torch.argmax(logits).item()\n",
                "    confidence = F.softmax(logits, dim=0)[pred_class].item()\n",
                "    \n",
                "    print(f\"\\nPrediction: {CLASS_NAMES[pred_class]} (confidence: {confidence:.2%})\")\n",
                "    \n",
                "    # ========================================\n",
                "    # 3. Fuse Attentions\n",
                "    # ========================================\n",
                "    print(\"\\nFusing attention maps...\")\n",
                "    fused_attentions = {}\n",
                "    for method in fusion_methods:\n",
                "        fused = fuse_vit_gat_attention(vit_patch_attention, gat_node_attention, \n",
                "                                       fusion_method=method, alpha=0.5)\n",
                "        fused_attentions[method] = fused\n",
                "        print(f\"  {method.capitalize()} fusion range: [{fused.min():.4f}, {fused.max():.4f}]\")\n",
                "    \n",
                "    # ========================================\n",
                "    # 4. Create Comprehensive Visualization\n",
                "    # ========================================\n",
                "    print(\"\\nGenerating visualizations...\")\n",
                "    \n",
                "    # Load original image\n",
                "    img = Image.open(image_path).convert('RGB')\n",
                "    img_array = np.array(img)\n",
                "    \n",
                "    # Create figure with multiple subplots\n",
                "    num_fusions = len(fusion_methods)\n",
                "    fig = plt.figure(figsize=(20, 5 * (num_fusions + 2)))\n",
                "    gs = fig.add_gridspec(num_fusions + 2, 4, hspace=0.4, wspace=0.3)\n",
                "    \n",
                "    # Row 0: Original image and ViT attention\n",
                "    ax1 = fig.add_subplot(gs[0, 0])\n",
                "    ax1.imshow(img_array)\n",
                "    ax1.set_title(f'Original Image\\nTrue: {class_name} | Pred: {CLASS_NAMES[pred_class]} ({confidence:.2%})', \n",
                "                  fontsize=12, fontweight='bold')\n",
                "    ax1.axis('off')\n",
                "    \n",
                "    ax2 = fig.add_subplot(gs[0, 1])\n",
                "    scatter = ax2.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=vit_patch_attention, cmap='Reds', s=150, alpha=0.8,\n",
                "                         edgecolors='black', linewidths=1)\n",
                "    ax2.set_title('ViT Internal Attention\\n(CLS token → patches)', fontsize=12, fontweight='bold')\n",
                "    ax2.invert_yaxis()\n",
                "    plt.colorbar(scatter, ax=ax2, fraction=0.046, pad=0.04)\n",
                "    \n",
                "    # Overlay heatmap on original image (ViT)\n",
                "    ax3 = fig.add_subplot(gs[0, 2:])\n",
                "    ax3.imshow(img_array, alpha=0.5)\n",
                "    scatter = ax3.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=vit_patch_attention, cmap='Reds', s=200, alpha=0.9)\n",
                "    ax3.set_title('ViT Attention Overlay', fontsize=12, fontweight='bold')\n",
                "    ax3.axis('off')\n",
                "    plt.colorbar(scatter, ax=ax3, fraction=0.046, pad=0.04)\n",
                "    \n",
                "    # Row 1: GAT attention\n",
                "    ax4 = fig.add_subplot(gs[1, 0])\n",
                "    scatter = ax4.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=gat_node_attention, cmap='Blues', s=150, alpha=0.8,\n",
                "                         edgecolors='black', linewidths=1)\n",
                "    ax4.set_title('GAT Node Attention\\n(Avg of edge attentions)', fontsize=12, fontweight='bold')\n",
                "    ax4.invert_yaxis()\n",
                "    plt.colorbar(scatter, ax=ax4, fraction=0.046, pad=0.04)\n",
                "    \n",
                "    ax5 = fig.add_subplot(gs[1, 1])\n",
                "    scatter = ax5.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=gat_global_attention, cmap='Greens', s=150, alpha=0.8,\n",
                "                         edgecolors='black', linewidths=1)\n",
                "    ax5.set_title('GAT Global Pooling\\n(Final classification)', fontsize=12, fontweight='bold')\n",
                "    ax5.invert_yaxis()\n",
                "    plt.colorbar(scatter, ax=ax5, fraction=0.046, pad=0.04)\n",
                "    \n",
                "    # Overlay heatmap on original image (GAT)\n",
                "    ax6 = fig.add_subplot(gs[1, 2:])\n",
                "    ax6.imshow(img_array, alpha=0.5)\n",
                "    scatter = ax6.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                         c=gat_node_attention, cmap='Blues', s=200, alpha=0.9)\n",
                "    ax6.set_title('GAT Node Attention Overlay', fontsize=12, fontweight='bold')\n",
                "    ax6.axis('off')\n",
                "    plt.colorbar(scatter, ax=ax6, fraction=0.046, pad=0.04)\n",
                "    \n",
                "    # Rows 2+: Fused attentions\n",
                "    for idx, (method, fused_att) in enumerate(fused_attentions.items()):\n",
                "        row = idx + 2\n",
                "        \n",
                "        # Scatter plot\n",
                "        ax_scatter = fig.add_subplot(gs[row, 0])\n",
                "        scatter = ax_scatter.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                                    c=fused_att, cmap='hot', s=150, alpha=0.8,\n",
                "                                    edgecolors='black', linewidths=1)\n",
                "        ax_scatter.set_title(f'Fused Attention ({method.capitalize()})\\n(ViT ⊗ GAT)', \n",
                "                            fontsize=12, fontweight='bold')\n",
                "        ax_scatter.invert_yaxis()\n",
                "        plt.colorbar(scatter, ax=ax_scatter, fraction=0.046, pad=0.04)\n",
                "        \n",
                "        # Heatmap on original\n",
                "        ax_overlay = fig.add_subplot(gs[row, 1:3])\n",
                "        ax_overlay.imshow(img_array, alpha=0.4)\n",
                "        scatter = ax_overlay.scatter(coordinates[:, 0], coordinates[:, 1], \n",
                "                                    c=fused_att, cmap='hot', s=250, alpha=0.95)\n",
                "        ax_overlay.set_title(f'Fused Attribution Map ({method.capitalize()})', \n",
                "                            fontsize=12, fontweight='bold')\n",
                "        ax_overlay.axis('off')\n",
                "        plt.colorbar(scatter, ax=ax_overlay, fraction=0.046, pad=0.04)\n",
                "        \n",
                "        # Top-k patches\n",
                "        ax_top = fig.add_subplot(gs[row, 3])\n",
                "        top_k = 10\n",
                "        top_indices = np.argsort(fused_att)[-top_k:][::-1]\n",
                "        colors = plt.cm.hot(np.linspace(0.9, 0.3, top_k))\n",
                "        \n",
                "        ax_top.barh(range(top_k), fused_att[top_indices], color=colors)\n",
                "        ax_top.set_yticks(range(top_k))\n",
                "        ax_top.set_yticklabels([f\"Patch {i}\" for i in top_indices])\n",
                "        ax_top.set_xlabel('Attention Score')\n",
                "        ax_top.set_title(f'Top-{top_k} Important Patches', fontsize=10, fontweight='bold')\n",
                "        ax_top.invert_yaxis()\n",
                "        ax_top.grid(axis='x', alpha=0.3)\n",
                "    \n",
                "    plt.suptitle(f'Multi-Level Attention Analysis: {class_name}', \n",
                "                 fontsize=16, fontweight='bold', y=0.995)\n",
                "    \n",
                "    save_path = f'attention_fusion_{class_name}.png'\n",
                "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\n✓ Visualization saved: {save_path}\")\n",
                "    \n",
                "    # ========================================\n",
                "    # 5. Print Detailed Statistics\n",
                "    # ========================================\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(\"ATTENTION STATISTICS\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    print(f\"\\n1. ViT Internal Attention:\")\n",
                "    print(f\"   Mean: {vit_patch_attention.mean():.4f}\")\n",
                "    print(f\"   Std:  {vit_patch_attention.std():.4f}\")\n",
                "    print(f\"   Top-5 patches: {np.argsort(vit_patch_attention)[-5:][::-1]}\")\n",
                "    \n",
                "    print(f\"\\n2. GAT Node Attention:\")\n",
                "    print(f\"   Mean: {gat_node_attention.mean():.4f}\")\n",
                "    print(f\"   Std:  {gat_node_attention.std():.4f}\")\n",
                "    print(f\"   Top-5 patches: {np.argsort(gat_node_attention)[-5:][::-1]}\")\n",
                "    \n",
                "    print(f\"\\n3. Fused Attentions:\")\n",
                "    for method, fused_att in fused_attentions.items():\n",
                "        print(f\"   {method.capitalize()}:\")\n",
                "        print(f\"     Mean: {fused_att.mean():.4f}\")\n",
                "        print(f\"     Std:  {fused_att.std():.4f}\")\n",
                "        print(f\"     Top-5: {np.argsort(fused_att)[-5:][::-1]}\")\n",
                "    \n",
                "    print(f\"\\n4. Graph Structure:\")\n",
                "    print(f\"   Threshold: {attention_info['threshold']:.4f}\")\n",
                "    print(f\"   Edges kept: {np.mean(attention_info['num_edges_kept']):.0f}\")\n",
                "    print(f\"   Sparsity: {(1 - np.mean(attention_info['num_edges_kept']) / len(attention_info['edge_masks_layer2'][0])):.2%}\")\n",
                "    \n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    return {\n",
                "        'vit_attention': vit_patch_attention,\n",
                "        'gat_attention': gat_node_attention,\n",
                "        'gat_global': gat_global_attention,\n",
                "        'fused_attentions': fused_attentions,\n",
                "        'prediction': CLASS_NAMES[pred_class],\n",
                "        'confidence': confidence,\n",
                "        'coordinates': coordinates\n",
                "    }\n",
                "\n",
                "print(\"Fused attention visualization function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6fd364a6",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 47 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15938cc7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize ViT with attention extraction capability\n",
                "print(\"Initializing ViT with attention extraction...\")\n",
                "vit_with_attention = ViTFeatureExtractorWithAttention().to(device)\n",
                "print(\"✓ Ready to extract multi-level attention!\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "51322979",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 48 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5eefd23",
            "metadata": {},
            "outputs": [],
            "source": [
                "# RERUN: Complete Fused Attention Extraction Pipeline\n",
                "print(\"=\"*80)\n",
                "print(\"INITIALIZING AND RUNNING FUSED ATTENTION EXTRACTION\")\n",
                "print(\"=\"*80 + \"\\n\")\n",
                "\n",
                "# Step 1: Initialize ViT with attention extraction\n",
                "print(\"Step 1: Initializing ViT with attention extraction...\")\n",
                "vit_with_attention = ViTFeatureExtractorWithAttention().to(device)\n",
                "print(\"✓ ViT initialized!\\n\")\n",
                "\n",
                "# Step 2: Run fused attention extraction on all classes\n",
                "print(\"Step 2: Extracting fused attention for all classes...\")\n",
                "attention_results = {}\n",
                "\n",
                "for class_name in CLASS_NAMES:\n",
                "    class_dir = os.path.join(TRAIN_DIR, class_name)\n",
                "    if os.path.exists(class_dir):\n",
                "        images = [f for f in os.listdir(class_dir) if f.endswith(('.tif', '.png', '.jpg'))]\n",
                "        if images:\n",
                "            # Take the first image as sample\n",
                "            sample_image = os.path.join(class_dir, images[10])\n",
                "            \n",
                "            # Extract and visualize fused attention\n",
                "            result = visualize_fused_attention(\n",
                "                model=model,\n",
                "                vit_with_attention=vit_with_attention,\n",
                "                image_path=sample_image,\n",
                "                class_name=class_name,\n",
                "                device=device,\n",
                "                fusion_methods=['additive', 'multiplicative', 'weighted']\n",
                "            )\n",
                "            \n",
                "            attention_results[class_name] = result\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"✓ COMPLETED: Fused attention analysis for all classes\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d2f8328",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 49 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce7d1473",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on one sample from each class\n",
                "print(\"=\"*80)\n",
                "print(\"TESTING FUSED ATTENTION EXTRACTION\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "attention_results = {}\n",
                "\n",
                "for class_name in CLASS_NAMES:\n",
                "    class_dir = os.path.join(TRAIN_DIR, class_name)\n",
                "    if os.path.exists(class_dir):\n",
                "        images = [f for f in os.listdir(class_dir) if f.endswith(('.tif', '.png', '.jpg'))]\n",
                "        if images:\n",
                "            # Take the first image as sample\n",
                "            sample_image = os.path.join(class_dir, images[0])\n",
                "            \n",
                "            # Extract and visualize fused attention\n",
                "            result = visualize_fused_attention(\n",
                "                model=model,\n",
                "                vit_with_attention=vit_with_attention,\n",
                "                image_path=sample_image,\n",
                "                class_name=class_name,\n",
                "                device=device,\n",
                "                fusion_methods=['additive', 'multiplicative', 'weighted']\n",
                "            )\n",
                "            \n",
                "            attention_results[class_name] = result\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"✓ COMPLETED: Fused attention analysis for all classes\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ad44d64b",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 50 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dc381a71",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare attention patterns across classes\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"CROSS-CLASS ATTENTION ANALYSIS\")\n",
                "print(\"=\"*80 + \"\\n\")\n",
                "\n",
                "comparison_data = {\n",
                "    'class': [],\n",
                "    'vit_mean': [],\n",
                "    'vit_std': [],\n",
                "    'gat_mean': [],\n",
                "    'gat_std': [],\n",
                "    'fusion_mult_mean': [],\n",
                "    'fusion_mult_std': []\n",
                "}\n",
                "\n",
                "for class_name, result in attention_results.items():\n",
                "    comparison_data['class'].append(class_name)\n",
                "    comparison_data['vit_mean'].append(result['vit_attention'].mean())\n",
                "    comparison_data['vit_std'].append(result['vit_attention'].std())\n",
                "    comparison_data['gat_mean'].append(result['gat_attention'].mean())\n",
                "    comparison_data['gat_std'].append(result['gat_attention'].std())\n",
                "    comparison_data['fusion_mult_mean'].append(result['fused_attentions']['multiplicative'].mean())\n",
                "    comparison_data['fusion_mult_std'].append(result['fused_attentions']['multiplicative'].std())\n",
                "\n",
                "# Create comparison plot\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "x_pos = np.arange(len(comparison_data['class']))\n",
                "width = 0.35\n",
                "\n",
                "# ViT Attention\n",
                "axes[0].bar(x_pos, comparison_data['vit_mean'], width, \n",
                "           yerr=comparison_data['vit_std'], capsize=5, \n",
                "           color='coral', alpha=0.8, edgecolor='black')\n",
                "axes[0].set_xlabel('Class', fontweight='bold')\n",
                "axes[0].set_ylabel('Mean Attention', fontweight='bold')\n",
                "axes[0].set_title('ViT Internal Attention by Class', fontweight='bold')\n",
                "axes[0].set_xticks(x_pos)\n",
                "axes[0].set_xticklabels(comparison_data['class'], rotation=45)\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# GAT Attention\n",
                "axes[1].bar(x_pos, comparison_data['gat_mean'], width, \n",
                "           yerr=comparison_data['gat_std'], capsize=5, \n",
                "           color='skyblue', alpha=0.8, edgecolor='black')\n",
                "axes[1].set_xlabel('Class', fontweight='bold')\n",
                "axes[1].set_ylabel('Mean Attention', fontweight='bold')\n",
                "axes[1].set_title('GAT Node Attention by Class', fontweight='bold')\n",
                "axes[1].set_xticks(x_pos)\n",
                "axes[1].set_xticklabels(comparison_data['class'], rotation=45)\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Fused Attention\n",
                "axes[2].bar(x_pos, comparison_data['fusion_mult_mean'], width, \n",
                "           yerr=comparison_data['fusion_mult_std'], capsize=5, \n",
                "           color='lightgreen', alpha=0.8, edgecolor='black')\n",
                "axes[2].set_xlabel('Class', fontweight='bold')\n",
                "axes[2].set_ylabel('Mean Attention', fontweight='bold')\n",
                "axes[2].set_title('Fused Attention (Multiplicative) by Class', fontweight='bold')\n",
                "axes[2].set_xticks(x_pos)\n",
                "axes[2].set_xticklabels(comparison_data['class'], rotation=45)\n",
                "axes[2].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('attention_comparison_across_classes.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n✓ Cross-class comparison saved: attention_comparison_across_classes.png\\n\")\n",
                "\n",
                "# Print summary table\n",
                "print(\"Summary Table:\")\n",
                "print(\"-\" * 80)\n",
                "print(f\"{'Class':<12} | {'ViT Mean':<10} | {'GAT Mean':<10} | {'Fused Mean':<10} | {'Prediction':<10}\")\n",
                "print(\"-\" * 80)\n",
                "for class_name, result in attention_results.items():\n",
                "    print(f\"{class_name:<12} | {result['vit_attention'].mean():<10.4f} | \"\n",
                "          f\"{result['gat_attention'].mean():<10.4f} | \"\n",
                "          f\"{result['fused_attentions']['multiplicative'].mean():<10.4f} | \"\n",
                "          f\"{result['prediction']:<10}\")\n",
                "print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "16625921",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 51 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following markdown cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2bb50c8",
            "metadata": {},
            "source": [
                "## 15. Clinical-Grade Heatmap Generation\n",
                "\n",
                "Generate high-quality, interpretable heatmaps suitable for clinical interpretation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85c46e6d",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 52 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db34a29b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.interpolate import griddata\n",
                "from matplotlib.colors import LinearSegmentedColormap\n",
                "\n",
                "def generate_smooth_heatmap(image_path, coordinates, attention_scores, \n",
                "                            output_path=None, alpha=0.6, method='cubic'):\n",
                "    \"\"\"\n",
                "    Generate smooth, interpolated heatmap overlay on original image.\n",
                "    \n",
                "    Args:\n",
                "        image_path: Path to original image\n",
                "        coordinates: (N, 2) patch center coordinates\n",
                "        attention_scores: (N,) attention values\n",
                "        output_path: Where to save the result\n",
                "        alpha: Transparency of heatmap overlay\n",
                "        method: Interpolation method ('linear', 'cubic', 'nearest')\n",
                "    \"\"\"\n",
                "    # Load image\n",
                "    img = Image.open(image_path).convert('RGB')\n",
                "    img_array = np.array(img)\n",
                "    height, width = img_array.shape[:2]\n",
                "    \n",
                "    # Create grid for interpolation\n",
                "    grid_x, grid_y = np.mgrid[0:width:1, 0:height:1]\n",
                "    \n",
                "    # Interpolate attention scores to create smooth heatmap\n",
                "    heatmap = griddata(\n",
                "        points=coordinates,\n",
                "        values=attention_scores,\n",
                "        xi=(grid_x, grid_y),\n",
                "        method=method,\n",
                "        fill_value=0\n",
                "    ).T\n",
                "    \n",
                "    # Normalize heatmap\n",
                "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
                "    \n",
                "    # Create figure\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
                "    \n",
                "    # Original image\n",
                "    axes[0].imshow(img_array)\n",
                "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
                "    axes[0].axis('off')\n",
                "    \n",
                "    # Heatmap only\n",
                "    im = axes[1].imshow(heatmap, cmap='hot', interpolation='bilinear')\n",
                "    axes[1].set_title('Attention Heatmap', fontsize=14, fontweight='bold')\n",
                "    axes[1].axis('off')\n",
                "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
                "    \n",
                "    # Overlay\n",
                "    axes[2].imshow(img_array, alpha=1.0)\n",
                "    im2 = axes[2].imshow(heatmap, cmap='hot', alpha=alpha, interpolation='bilinear')\n",
                "    axes[2].set_title('Heatmap Overlay (Attribution Map)', fontsize=14, fontweight='bold')\n",
                "    axes[2].axis('off')\n",
                "    plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    \n",
                "    if output_path:\n",
                "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
                "        print(f\"✓ Saved: {output_path}\")\n",
                "    \n",
                "    plt.show()\n",
                "    \n",
                "    return heatmap\n",
                "\n",
                "print(\"Smooth heatmap generator defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e7538602",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 53 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aed891c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate smooth heatmaps for all classes and fusion methods\n",
                "print(\"=\"*80)\n",
                "print(\"GENERATING CLINICAL-GRADE HEATMAPS\")\n",
                "print(\"=\"*80 + \"\\n\")\n",
                "\n",
                "for class_name, result in attention_results.items():\n",
                "    print(f\"\\nProcessing {class_name}...\")\n",
                "    \n",
                "    # Find the original image path\n",
                "    class_dir = os.path.join(TRAIN_DIR, class_name)\n",
                "    images = [f for f in os.listdir(class_dir) if f.endswith(('.tif', '.png', '.jpg'))]\n",
                "    sample_image = os.path.join(class_dir, images[0])\n",
                "    \n",
                "    coordinates = result['coordinates']\n",
                "    \n",
                "    # Generate heatmaps for different attention sources\n",
                "    print(f\"  1. ViT Internal Attention...\")\n",
                "    generate_smooth_heatmap(\n",
                "        sample_image, coordinates, result['vit_attention'],\n",
                "        output_path=f'heatmap_vit_{class_name}.png',\n",
                "        alpha=0.6\n",
                "    )\n",
                "    \n",
                "    print(f\"  2. GAT Node Attention...\")\n",
                "    generate_smooth_heatmap(\n",
                "        sample_image, coordinates, result['gat_attention'],\n",
                "        output_path=f'heatmap_gat_{class_name}.png',\n",
                "        alpha=0.6\n",
                "    )\n",
                "    \n",
                "    print(f\"  3. Fused Attention (Multiplicative)...\")\n",
                "    generate_smooth_heatmap(\n",
                "        sample_image, coordinates, result['fused_attentions']['multiplicative'],\n",
                "        output_path=f'heatmap_fused_multiplicative_{class_name}.png',\n",
                "        alpha=0.7\n",
                "    )\n",
                "    \n",
                "    print(f\"  4. Fused Attention (Additive)...\")\n",
                "    generate_smooth_heatmap(\n",
                "        sample_image, coordinates, result['fused_attentions']['additive'],\n",
                "        output_path=f'heatmap_fused_additive_{class_name}.png',\n",
                "        alpha=0.7\n",
                "    )\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"✓ ALL HEATMAPS GENERATED\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a95a67f",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 54 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "031ce557",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "38d2f2c6",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "**Cell 55 — Purpose & Usage**\n",
                "\n",
                "Configuration: Uses prior definitions and variables in the notebook.\n",
                "\n",
                "Explanation: The following code cell performs the next logical step in the pipeline (see its code/content).\n",
                "\n",
                "Use: Run this cell in sequence; intended to be executed after earlier setup cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82dfd50c",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "clam_latest",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
